"""
–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
"""

import os
import sys
import argparse
import json
import numpy as np

sys.path.append('src')

from data_preprocessing import FieldSegmentationPreprocessor
from model import UNetSegmentation, SegmentationDataGenerator


def parse_args():
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏')
    parser.add_argument('--data_dir', type=str, default='data/raw/EuroSAT')
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--encoder', type=str, default='resnet34',
                       choices=['resnet34', 'resnet50', 'efficientnetb3'])
    parser.add_argument('--learning_rate', type=float, default=0.0001)
    return parser.parse_args()


def main():
    args = parse_args()
    
    print("=" * 80)
    print("üõ∞Ô∏è  –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–ò –ü–û–õ–ï–ô")
    print("=" * 80)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists('data/processed/config.json'):
        preprocessor = FieldSegmentationPreprocessor(
            data_dir=args.data_dir,
            img_size=(256, 256),
            batch_size=args.batch_size
        )
        train_df, val_df, test_df = preprocessor.prepare_pipeline()
    else:
        import pandas as pd
        print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        train_df = pd.read_csv('data/processed/train.csv')
        val_df = pd.read_csv('data/processed/val.csv')
        test_df = pd.read_csv('data/processed/test.csv')
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    with open('data/processed/config.json', 'r') as f:
        config = json.load(f)
    
    num_classes = config['num_classes']
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤
    print("\nüîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
    train_gen = SegmentationDataGenerator(
        train_df, args.batch_size, (256, 256), num_classes, augment=True
    )
    val_gen = SegmentationDataGenerator(
        val_df, args.batch_size, (256, 256), num_classes, augment=False
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nüèóÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ U-Net –º–æ–¥–µ–ª–∏...")
    model_builder = UNetSegmentation(
        num_classes=num_classes,
        img_size=(256, 256),
        encoder=args.encoder
    )
    
    model = model_builder.build_model()
    model_builder.compile_model(learning_rate=args.learning_rate)
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {args.encoder}")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_params():,}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    history = model_builder.train(
        train_gen=train_gen,
        val_gen=val_gen,
        epochs=args.epochs
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    model_builder.save_model('models/final_model.h5')
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
    history_dict = {k: [float(v) for v in vals] 
                   for k, vals in history.history.items()}
    with open('reports/training_history.json', 'w') as f:
        json.dump(history_dict, f, indent=4)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
    model_config = {
        'encoder': args.encoder,
        'num_classes': num_classes,
        'img_size': 256,
        'batch_size': args.batch_size,
        'epochs': args.epochs,
        'class_names': config['class_names']
    }
    
    with open('models/config.json', 'w') as f:
        json.dump(model_config, f, indent=4)
    
    print("\n" + "=" * 80)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 80)
    
    best_iou = max(history.history['val_iou_score'])
    best_fscore = max(history.history['val_f1-score'])
    
    print(f"\nüìà –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   Val IoU Score: {best_iou:.4f}")
    print(f"   Val F1-Score: {best_fscore:.4f}")
    
    print(f"\nüéØ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:")
    print("   python src/evaluate.py")


if __name__ == "__main__":
    main()
